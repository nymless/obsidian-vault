Пусть есть выборка $F$, состоящая из $l$ объектов и $n$ признаков:$$F_{l\times n} = \begin{pmatrix} x_1\\...\\x_l \end{pmatrix} = \begin{pmatrix} x_{11} & ... & x_{1n}\\ ...&...&...\\ x_{l1} & ... & x_{ln} \end{pmatrix},$$где $x_i$ - вектор признакового описания $i$-го объекта.

**Задача**:
Создать новые признаки $z$ на основе старых $x$ таким образом, чтобы выполнялись следующие требования:
- Новых признаков должно быть **не больше** чем старых признаков: $m \leq n$;
- Старые признаки должны **линейно восстанавливаться** по новым признакам:$$\hat{x}_{ij}=\sum_{k=1}^m z_{ik} u_{jk},\quad j=\overline{1,n},$$где $\hat{x}_{ij}$ - приближение $j$-го истинного признака $i$-го объекта при помощи линейной комбинации новых признаков $i$-го объекта; $u_{jk}$ - коэффициенты при линейной комбинации;
- Новые признаки должны как можно точнее описывать старые. Запишем это условие, применяя **метод наименьших квадратов** (МНК):$$\sum_{i=1}^l \sum_{j=1}^n (\hat{x}_{ij}-x_{ij})^2 =$$$$= \sum_{i=1}^l \sum_{j=1}^n \Big(\sum_{k=1}^m z_{ik} u_{jk} - x_{ij} \Big)^2 \rightarrow \min_{g,u}.$$

Пусть $G$ - матрица новых признаков:$$G_{l\times m} = \begin{pmatrix} z_1\\...\\z_l \end{pmatrix} = \begin{pmatrix} z_{11} & ... & z_{1m}\\ ...&...&...\\ z_{l1} & ... & z_{lm} \end{pmatrix}$$Пусть $U$ - матрица коэффициентов при линейных комбинациях:$$U_{n\times m} = \begin{pmatrix} u_{11} & ... & u_{1m}\\ ...&...&...\\ u_{n1} & ... & u_{nm} \end{pmatrix}$$Тогда условие МНК можно записать в матричном виде минимизацией функционала:$$Q(G,U) = \sum_{i=1}^l \lVert z_i U^\top - x_i \rVert^2 = \lVert G U^\top - F \rVert^2 \rightarrow \min_{G,U}$$Исчерпывающее решение задачи даёт следующая **теорема**:

Если $m \leq \text{rank}\ F$, то минимум функционала $Q(G,U)$ достигается, когда столбцы матрицы $U$ есть собственные векторы матрицы $F^\top F$, соответствующие $m$ максимальным собственным значениям, а матрица $G = FU$.

При этом:
- $U$ - [[Ортогональная матрица (Orthogonal matrix)|ортогональная матрица]]. Выполняется условие: $U^\top U = I$, где $I$ - единичная матрица. Это значит, что в выражении $G = FU$, матрица $U$ производит ортогонализирующее преобразование матрицы исходных признаков $F$;
- $G$ - **матрица с ортогональными столбцами**. Выполняется условие: $G^\top G = \Lambda = \text{diag}\ (\lambda_1,...,\lambda_m)$, где $\Lambda$ - диагональная матрица, содержащая [[Собственные векторы и собственные значения линейного оператора (Eigenvectors and Eigenvalues)|собственные числа]] матрицы $G^\top G$ на главной диагонали. Собственные числа матрицы $G^\top G$ являются [[Сингулярные числа и сингулярные векторы (Singular values and Singular vectors)|сингулярными числами]] матрицы $G$. Связь матриц $G$, $F$ и $\Lambda$ выражается:$$G^\top G = U^\top F^\top F U = \Lambda;$$
- $U\Lambda = F^\top F U$ - матрица $U$ по столбцам состоит из собственных векторов матрицы $F^\top F$, или **правых сингулярных векторов** матрицы $F$;
- $G\Lambda = F F^\top G$ - матрица $G$ по столбцам состоит из собственных векторов матрицы $F F^\top$, или **левых сингулярных векторов** матрицы $F$;
- Линейное преобразование $U$ работает в обе стороны:$$\hat{F} = GU^\top,\quad G = FU.$$

Представим матрицу $G$ в виде произведения ортогональной матрица $V$ и диагональной матрицы $D = \sqrt\Lambda$, у которой на главной диагонали сингулярные числа матрицы $G$ (которые ещё являются сингулярными числами матрицы приближений $\hat{F}$). Тогда [[Сингулярное разложение матриц (Singular value decomposition)|сингулярное разложение]] матрицы $\hat{F}$ имеет вид:$$\hat{F} = GU^\top = V \sqrt\Lambda U^\top = VDU^\top;$$Если вычислить сингулярные числа матрицы $F$, то при помощи следующего выражения можно оценить величину невязки:$$\lVert GU^\top - F \rVert^2 = \lVert F \rVert^2 - \text{tr}\ \Lambda = \text{tr}\ (F^\top F) - \text{tr}\ \Lambda =$$$$= \sum_{j=1}^n \lambda_j - \sum_{j=1}^m \lambda_j = \sum_{j=m+1}^n \lambda_j;$$Оценку невязки можно использовать для построения графика отношения невязки к истинным значениям:$$E_m = \frac{\lVert GU^\top - F \rVert^2}{\lVert F \rVert^2} = \frac{\lambda_{m+1}+...+\lambda_n}{\lambda_1+...+\lambda_n}$$Далее по критерию «**крутого склона**» можно оценить **эффективную размерность** выборки.

На практике, вместо матрицы $F^\top F$ часто используют **выборочную ковариационную матрицу**:$$\Sigma = \frac{1}{l-1} \sum_{k=1}^l (x_{ki} - \overline{X}_i)(x_{kj} - \overline{X}_j)$$При таком подходе сохраняются все этапы метода PCA, а абсолютные значения чисел при вычислениях будут меньше.