Норма вектора $w$ минимальна там же, где минимальна его [[Квадратичная форма (Quadratic form)|квадратичная форма]] вида $\langle w,w \rangle$. Построение оптимальной разделяющей гиперплоскости сводится к минимизации квадратичной формы при $m$ ограничениях-неравенствах относительно переменных $w$, $w_0$:$$\begin{cases} \langle w,w \rangle \rightarrow \min \\ y_i \big( \langle w,x_i \rangle - w_0 \big) \geq 1, \quad i=\overline{1,m}. \end{cases}$$По [[Теорема Куна-Таккера|теореме Куна-Таккера]] эта задача эквивалентна двойственной задаче поиска **седловой точки Лагранжа**:$$\begin{cases}
\mathcal{L}(w,w_0;\lambda) = \frac{1}{2}\langle w,w \rangle - \\ 
-\sum_{i=1}^m \lambda_i \Big( y_i \big( \langle w,x_i \rangle - w_0 \big) - 1 \Big) \rightarrow \underset{w,w_o}{\min} \underset{\lambda}{\max}; \\
\lambda_i = 0, \quad \text{либо} \quad \langle w,x_i \rangle - w_0 = y_i, \quad i = \overline{1,m}; \\
\lambda_i \geq 0, \quad i=\overline{1,m}; \\
\end{cases}$$где $\lambda = (\lambda_1,...,\lambda_m)$ - вектор двойственных переменных. Второе условие называется [[Условия Каруша-Куна-Таккера (Karush-Kuhn-Tucker conditions)|условием дополняющей нежёсткости]].

Далее находятся частные производные [[Функция Лагранжа|лагранжиана]], из которых оказывается, что вектор переменных $w$ является линейной комбинацией обучающей выборки $x_i$, $y_i$, причём только тех, для которых $\lambda_i \neq 0$:$$w=\sum_{i=1}^m \lambda_i y_i x_i.$$Из условия $\lambda_i \neq 0$, а также из [[Условия Каруша-Куна-Таккера (Karush-Kuhn-Tucker conditions)|условий]] дополняющей нежёсткости и неотрицательности, делается вывод, что алгоритм зависит только от векторов $x_i$, для которых выполняется $\lambda_i > 0$ и $\langle w,x_i \rangle - w_0 = y_i$. Векторы, которые отвечают этим условиям, лежат на границе разделяющей полосы и называются **опорными векторами** (**support vectors**).

Далее частные производные подставляются в **лагранжиан** и решается эквивалентная задача квадратичного программирования, содержащая только двойственные переменные $\lambda_i$. Получив решение задачи, можно найти переменные $w,w_0$.

Решение задачи разделения линейно разделимой выборки методом SVM можно записать в **следующем виде**:$$a(x)=\text{sign} \Big( \sum_{i=1}^m \lambda_i y_i \langle x_i,x \rangle - w_0 \Big).$$Реально суммирование идёт не по всей выборке, а только по опорным векторам, для которых $\lambda_i \neq 0$. Такое свойство метода SVM принято называть **разреженностью** (**sparsity**)