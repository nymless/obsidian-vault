Для обобщения на случай линейной неразделимости, алгоритму позволяется допускать минимальные ошибки на обучающих объектах $x_i$. Введём набор дополнительных переменных $\xi_i \geq 0$, характеризующих величину ошибки, которые введём в минимизируемый функционал $\lVert w,w \rVert$. Тогда задача сводится к следующему:$$\begin{cases}
\frac{1}{2}\langle w,w \rangle + С \sum_{i=1}^m \xi_i \rightarrow \underset{w,w_0,\xi}{\min};\\
M_i = y_i\big( \langle w,x_i \rangle - w_0 \big) \geq 1 - \xi_i, \quad i=\overline{1,m};\\
\xi_i \geq 0, \quad i=\overline{1,m},
\end{cases}$$где $C > 0$ - некоторая числовая константа.

Таким образом, положительная константа $C$ является **гиперпараметром** метода SVM и позволяет находить компромисс между **максимизацией** разделяющей полосы $M_i \rightarrow \max \Leftrightarrow \langle w,w \rangle \rightarrow \min$ и **минимизацией** суммарной ошибки $\sum_{i=1}^m \xi_i \rightarrow \min$.

Другой интерпретацией **гиперпараметра** $C$ является, то что это величина **обратная** [[Регуляризация по Тихонову (Tikhonov regularization)|параметру регуляризации по Тихонову]].

**Отступом** $M_i$ каждого объекта обучающей выборки $x_i$, называется величина:$$M_i = y_i \big( \langle w,x_i\rangle-w_0 \big).$$Алгоритм допускает ошибку на объекте $x_i$ тогда и только тогда, когда отступ $M_i$ отрицателен, то есть когда $M_i < 0$. Запишем функционал числа ошибок алгоритма на выборке $X^m$ как:$$Loss(X^m) = \sum_{i=1}^m[M_i < 0].$$Такую пороговую функцию можно заменить на её верхне-пороговую аппроксимацию, через кусочно-линейную «**шарнирную**» функцию потерь (**hinge loss**):$$Loss(X^m) = \sum_{i=1}^m(1 - M_i)_+$$Такая функция потерь штрафует алгоритм за величину ошибки, а не только за её наличие. Кроме того, в соответствии с методом **регуляризации по Тихонову**, добавим к функционалу штрафное слагаемое $\tau \lVert w \rVert^2$, где $\tau$ - параметр регуляризации. Такая добавка означает, что среди всех векторов $w$, минимизирующих функционал, наиболее предпочтительны векторы с минимальной нормой. Тогда функционал ошибки примет вид:$$Loss(X^m) = \sum_{i=1}^m (1 - M_i)_+ + \tau \lVert w \rVert^2 \rightarrow \min.$$Можно показать, что поставленная таким образом задача, эквивалентна оптимизационной задаче с ограничениями, если взять параметр регуляризации $\tau = \frac{1}{2C}$. Таким образом, **показано**, что гиперпараметр $C$ - величина обратная параметру регуляризации.

Запишем [[Функция Лагранжа|функцию Лагранжа]] для двойственной задачи оптимизации нашего функционала:$$\mathcal{L}(w,w_0,\xi;\lambda,\eta) = \frac{1}{2}\lVert w,w, \rVert- $$$$-\sum_{i=1}^m \lambda_i \Big( y_i \big( \langle w,x_i \rangle - w_0 \big) -1 \Big) - \sum_{i=1}^m \xi_i (\lambda_i + \eta_i - C),$$где $\eta_i=(\eta_1,...,\eta_m)$ - вектор переменных **двойственных** к переменным $\xi = (\xi_1,...,\xi_m)$. Как и в случае с [[Метод SVM для линейно разделимой выборки|линейно разделимой выборкой]], полученные [[Условия Каруша-Куна-Таккера (Karush-Kuhn-Tucker conditions)|условия Каруша-Куна-Таккера]] сводят задачу к поиску **седловой точки функции Лагранжа**:$$\begin{cases}
\mathcal{L}(w,w_o,\xi;\lambda,\eta) \rightarrow \underset{w,w_o,\xi}{\min} \underset{\lambda,\eta}{\max};\\
\xi_i \geq 0,\quad \lambda_i\geq 0,\quad \eta_i \geq 0,\quad i = \overline{1,m};\\
\lambda_i = 0,\quad\text{либо}\quad y_i\big(\langle w,w \rangle-w_0\big)=1-\xi_i,\quad i=\overline{1,m};\\
\eta_i = 0,\quad\text{либо}\xi_i=0,\quad i=\overline{1,m}.\\
\end{cases}$$ В последних двух строчках написаны условия дополняющей нежёсткости. Необходимым условием седловой точки является равенство нулю частных производных **лагранжиана**. Производные по $w$ и $w_0$ в точности соответствуют производным лагранжиана в линейно разделимом случае, тогда как производная по $\xi_i$ приводит к соотношению:$$\eta_i + \lambda_i = C,\quad i=\overline{1,m}.$$Из этого соотношения и неравенства $\eta_i \geq 0$ следует ограничение $\lambda_i \leq C$. Отсюда, и из условий дополняющей нежёсткости вытекает, что возможны только **три** допустимых сочетания значений переменных $\xi_i,\lambda_i,\eta_i$ и отступов $M_i$:
1. $\lambda_i=0$; $\eta_i=C$, $\xi_i=0$; $M_i>1$ - объект $x_i$ классифицируется правильно и находится далеко от разделяющей полосы.
2. $0<\lambda_i<C$; $=<\eta_i<C$; $\xi_i=0$; $M_i=1$ - объект $x_i$ классифицируется правильно и лежит в точности на границе разделяющей полосы.
3. $\lambda_i=C$; $\eta_i=0$; $\xi_i>0$; $M_i<1$ - объект $x_i$ либо лежит внутри разделяющей полосы, но классифицируется правильно ($0<\xi_i<1$, $0<M_i<1$), либо попадает на границу классов ($\xi_i=1$, $M_i=0$), либо вообще относится к чужому классу ($\xi_i>1$, $M_i<0$). Назовём их объектами **нарушителями**.

Далее частные производные подставляются в **лагранжиан** и решается эквивалентная задача квадратичного программирования, содержащая только двойственные переменные. При этом, в силу соотношения $\eta_i + \lambda_i = C$ в лагранжиане обнуляются все члены, содержащие переменные $\xi_i,\eta_i$ и он принимает тот же вид, что и в случае линейно разделимости.

Таким образом, задача решается **таким же** способом, что и в линейно разделимом случае, с той лишь разницей, что теперь ненулевыми $\lambda_i$ обладают не только **опорные** объекты, но и объекты **нарушители**:$$a(x)=\text{sign} \Big( \sum_{i=1}^m \lambda_i y_i \langle x_i,x \rangle - w_0 \Big).$$

**Заключение**:
Объекты **нарушители**, которые зачастую являются шумом в данных, влияют на финальное правило классификации. Это может оказаться серьёзным недостатком метода SVM и потребовать либо **фильтрации** шума, либо поиска альтернативных, более **устойчивых** (робастных) методов классификации.