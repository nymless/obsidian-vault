Для обобщения на случай линейной неразделимости, алгоритму позволяется допускать минимальные ошибки на обучающих объектах $x_i$. Введём набор дополнительных переменных $\xi_i \geq 0$, характеризующих величину ошибки, которые введём в минимизируемый функционал $\lVert w,w \rVert$. Тогда задача сводится к следующему:$$\begin{cases}
\frac{1}{2}\langle w,w \rangle + С \sum_{i=1}^m \xi_i \rightarrow \underset{w,w_0,\xi}{\min};\\
M_i = y_i\big( \langle w,x_i \rangle - w_0 \big) \geq 1 - \xi_i, \quad i=\overline{1,m};\\
\xi_i \geq 0, \quad i=\overline{1,m},
\end{cases}$$где $C > 0$ - некоторая числовая константа.

Таким образом, положительная константа $C$ является **гиперпараметром** метода SVM и позволяет находить компромисс между **максимизацией** разделяющей полосы $M_i \rightarrow \max \Leftrightarrow \langle w,w \rangle \rightarrow \min$ и **минимизацией** суммарной ошибки $\sum_{i=1}^m \xi_i \rightarrow \min$.

Другой интерпретацией **гиперпараметра** $C$ является, то что это величина **обратная** [[Регуляризация по Тихонову (Tikhonov regularization)|параметру регуляризации по Тихонову]]. **Отступом** $M_i$ каждого объекта обучающей выборки $x_i$, называется величина:$$M_i = y_i \big( \langle w,x_i\rangle-w_0 \big).$$Алгоритм допускает ошибку на объекте $x_i$ тогда и только тогда, когда отступ $M_i$ отрицателен, то есть когда $M_i < 0$. Запишем функционал числа ошибок алгоритма на выборке $X^m$ как:$$Loss(X^m) = \sum_{i=1}^m[M_i < 0].$$Такую пороговую функцию можно заменить её верхне-пороговой аппроксимацией, через кусочно-линейную «**шарнирную**» функцию потерь (**hinge loss**):$$Loss(X^m) = \sum_{i=1}^m(1 - M_i)_+$$Такая функция потерь штрафует алгоритм за величину ошибки, а не только за её наличие. Кроме того, в соответствии с методом **регуляризации по Тихонову**, добавим к функционалу штрафное слагаемое $\tau \lVert w \rVert^2$, где $\tau$ - параметр регуляризации. Такая добавка означает, что среди всех векторов $w$, минимизирующих функционал, наиболее предпочтительны векторы с минимальной нормой. Тогда функционал ошибки примет вид:$$Loss(X^m) = \sum_{i=1}^m (1 - M_i)_+ + \tau \lVert w \rVert^2 \rightarrow \min.$$Можно показать, что поставленная таким образом задача, эквивалентна оптимизационной задаче с ограничениями, если взять параметр регуляризации $\tau = \frac{1}{2C}$. Таким образом, **показано**, что гиперпараметр $C$ - величина обратная параметру регуляризации.