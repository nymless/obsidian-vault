Для обобщения на случай линейной неразделимости, алгоритму позволяется допускать минимальные ошибки на обучающих объектах $x_i$. Введём набор дополнительных переменных $\xi_i \geq 0$, характеризующих величину ошибки. Таким образом, с введённой ошибкой, построение оптимально разделяющей гиперплоскости привод к следующим [[Условия Каруша-Куна-Таккера (Karush-Kuhn-Tucker conditions)|условиям Каруша-Куна-Таккера]]:$$\begin{cases}
L(w,w_0;\lambda) = \frac{1}{2}\langle w,w \rangle + С \sum_{i=1}^m \xi_i \rightarrow \underset{w,w_0,\xi}{\min};\\
M_i = y_i\big( \langle w,x_i \rangle - w_0 \big) \geq 1 - \xi_i, \quad i=\overline{1,m};\\
\xi_i \geq 0, \quad i=\overline{1,m},
\end{cases}$$где $C > 0$ - некоторая числовая константа.

Таким образом, положительная константа $C$ является **гиперпараметром** метода SVM и позволяет находить компромисс между **максимизацией** разделяющей полосы $M_i \rightarrow \max \Leftrightarrow \langle w,w \rangle \rightarrow \min$ и **минимизацией** суммарной ошибки $\sum_{i=1}^m \xi_i \rightarrow \min$.