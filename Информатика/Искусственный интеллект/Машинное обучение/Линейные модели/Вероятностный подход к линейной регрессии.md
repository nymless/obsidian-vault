Пусть есть истинная функция $f(x)$, которой мы обозначим некоторый изучаемый нами закон в реальном мире. Однако, эмпирические измерения реальных процессов подвержены шуму $\varepsilon$ и реально нами наблюдаются значения, которые принимает функция $y(t)$ равная:
$$
y(t)=f(t)+\varepsilon
$$
где $\varepsilon$ - случайная величина. Предположим, что случайная величина имеет **нормальное распределение**: $\mathcal{N}(0,\sigma)$. Тогда можно считать, что значения $y_i$ функции $y(t_i)$ являются случайными величинами с **нормальным распределением**:
$$
y_i=y(t_i) \sim \mathcal{N}(f(t_i),\sigma)
$$
Оценим параметры [[Линейная регрессия|линейной регрессии]] $w=(w_0,w_1,...,w_D)$, используя метод наименьшего правдоподобия. **Функция правдоподобия** (**Likelihood**) имеет вид:
$$
\mathcal{L}(w;y) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi}\sigma} \exp \Big( -\frac{(y_i-f(t_i;w))^2}{2\sigma^2} \Big);
$$
$$
\ln \mathcal{L}(w;y) = \sum_{i=1}^N \ln \frac{1}{\sqrt{2\pi}\sigma} + \sum_{i=1}^N \Big( -\frac{(y_i-f(t_i;w))^2}{2\sigma^2} \Big)=
$$
$$
= \sum_{i=1}^N \ln \frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{2\sigma^2} \sum_{i=1}^N (y_i-f(t_i;w))^2 \xrightarrow{w} \max
$$
Оставив только часть, которая зависит от параметров $w$, и заметив, что правдоподобие максимально, когда эта часть минимальна, получим:
$$
\mathcal{L}(w;y) \xrightarrow{w} \max \Leftrightarrow \sum_{i=1}^N (y_i-f(t_i;w))^2 \xrightarrow{w} \min
$$
Таким образом, выбранная функция потерь исходит из предположения, что ошибка $\varepsilon$ имеет **нормальное распределение**. При наличие априорной информации о другом виде распределения, функция потерь будет иной.