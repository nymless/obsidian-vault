Пусть есть некоторый набор данных, состоящий из двух предикторов $x_1,x_2$ и $N=2$ разных классов. Первый класс представлен $l$ объектами, второй класс $m$ объектами.

**Допущения метода** LDA:
Предикторы являются случайной выборкой из двумерного **нормального** распределения. Классы имеют схожие дисперсионно-ковариационные матрицы (классы однородны, **гомоскедастичны**).

Требуется найти линейную комбинацию признаков (гиперплоскость), с помощью которой можно разделить два класса, или сократить размерность пространства. В геометрическом смысле, мы ищем некоторую гиперплоскость, на которую можно спроецировать пары признаков (векторы), принадлежащие объектам выборки, такую, чтобы классы можно было разделить оптимальным образом. Такая гиперплоскость имеет размерность $N-1$, а в нашем случае $1$ (линия).

Обозначим искомую одномерную линию вектором $\omega \in \mathbb{R}^2$, условимся, что вектор имеет норму $\lVert\omega\rVert=1$. Спроецируем все точки двух классов на линию. Обозначим проекции $y_i,y_j$, тогда справедливо:$$y_i=\omega^\top x_i,\quad y_j=\omega^\top x_j,\quad i=\overline{1,l},\quad j=\overline{1,m}.$$Обозначим центроиды двух классов (их средние) $\mu_1,\mu_2 \in \mathbb{R}^2$. Проекции векторов $\mu_1,\mu_2$ на вектор $\omega$ также равны скалярному произведению:$$\widetilde{\mu}_1=\omega^\top\mu_1,\quad \widetilde{\mu}_2=\omega^\top\mu_2.$$Расстояние между спроецированными центроидами можно характеризовать следующим выражением:$$( \widetilde{\mu}_1 - \widetilde{\mu}_2 )^2$$Чем **расстояние** между спроецированными центроидами **больше**, тем лучше для разделения классов. Однако, одного только этого недостаточно, нужно учитывать ещё и величину разброса классов в проекциях. Характеристикой разброса первого и второго класса можно считать выражения:$$\widetilde{S}_1=\sum_{i=1}^l(y_i-\widetilde{\mu}_1)^2;\quad \widetilde{S}_2=\sum_{j=1}^m(y_j-\widetilde{\mu}_2)^2.$$Общую характеристику разброса можно определить через сумму:$$\widetilde{S} = \widetilde{S}_1 + \widetilde{S}_2$$Чем **разброс** $\widetilde{S}$ **меньше**, тем лучше для разделения классов.

Таким образом, условие наилучшего разделения классов по их проекциям можно задать через максимизацию отношения:$$\frac{( \widetilde{\mu}_1 - \widetilde{\mu}_2 )^2}{\widetilde{S}_1 + \widetilde{S}_2} \rightarrow \max_\omega$$В числителе характеристика расстояния между проекциями центроид классов, которую требуется **максимизировать**, а в знаменателе характеристика разброса проекция точек в классах, которую требуется **минимизировать**. Распишем характеристики разброса классов:$$\widetilde{S}_1 = \sum_{i=1}^l(y_i-\widetilde{\mu}_1)^2 =$$$$= \sum_{i=1}^l(\omega^\top x_i-\omega^\top\mu_1)(\omega^\top x_i-\omega^\top\mu_1)=$$$$= \omega^\top \Big( \sum_{i=1}^l (x_i-\mu_1)(x_i-\mu_1)^\top \Big) \omega = \omega^\top S_1 \omega,$$где $S_1=\sum_{i=1}^l (x_i-\mu_1)(x_i-\mu_1)^\top$ - матрица $2 \times 2$ - характеристика разброса внутри первого класса. По аналогии, для второго класса:$$\widetilde{S}_2 = \omega^\top S_2 \omega.$$Обозначим $S_W$ - **матрица** $2 \times 2$ - характеристика разброса внутри двух классов (от слова **within** - внутри). Тогда:$$\widetilde{S}_1 + \widetilde{S}_2 = \omega^\top (S_1+S_2) \omega = \omega^\top S_W \omega.$$Преобразуем выражения расстояния между центроидами:$$(\widetilde{\mu}_1 - \widetilde{\mu}_2)^2 = (\omega^\top\mu_1 - \omega^\top\mu_2)(\omega^\top\mu_1 - \omega^\top\mu_2)=$$$$\omega^\top (\mu_1 - \mu_2) (\mu_1 - \mu_2)^\top \omega = \omega^\top S_B \omega$$где $S_B$ - **матрица** $2 \times 2$ - характеристика расстояния между классами (от слова **between** - между).

Таким образом, условие наилучшего разделения классов по их проекциям на $\omega$ определяется как максимум функционала $J$ по $\omega$:$$J(\omega) = \frac{\omega^\top S_B \omega}{\omega^\top S_W \omega} \rightarrow \max_\omega$$где $\omega^\top S_W \omega \neq 0$ - скаляр не равный нулю. Данное выражения называется - **критерий Фишера**.

Продифференцируем и приравняем производную к нулю:$$\frac{dJ}{d\omega} = \frac{2(S_B \omega) (\omega^\top S_W \omega) - 2(\omega^\top S_B \omega) (S_W \omega)}{(\omega^\top S_W \omega)^2} =$$$$= \frac{2}{\omega^\top S_W \omega} \Big( S_B\omega - J(\omega)S_W\omega \Big) = 0$$Известно, что $\omega^\top S_W \omega \neq 0$, следовательно задача сводится:$$S_B\omega = J(\omega)S_W\omega$$
**Предположим**, что матрица $S_W$ [[Невырожденная матрица (Invertible matrix)|обратима]]. Тогда умножив левую и правую части на $S_W^{-1}$ получим:$$S_W^{-1}S_B\omega = J(\omega)\omega$$Заметим, что значение $J(\omega)$ - это просто число. Обозначим его $\lambda$:$$S_W^{-1}S_B\omega = \lambda\omega$$Таким образом, из данного выражения можно сделать слeдующие выводы:
- $S_W^{-1}S_B$ - матрица [[Линейный оператор (Linear map, Linear mapping)|линейного оператора]], которую можно найти из исходных данных;
- $\omega$ - [[Собственные векторы и собственные значения линейного оператора (Eigenvectors and Eigenvalues)|собственный вектор]] матрицы $S_W^{-1}S_B$;
- $\lambda$ - [[Собственные векторы и собственные значения линейного оператора (Eigenvectors and Eigenvalues)|собственное значение]] матрицы $S_W^{-1}S_B$;
- так как мы максимизируем функционал $J(\omega)$, то нас интересует **собственный вектор** $\omega$ матрицы $S_W^{-1}S_B$, который соответствует **максимальному** собственному значению.

**Примечание**:
Если матрица $S_W$ **необратима** то можно использовать [[Псевдообратная матрица (Moore-Penrose inverse, Pseudoinverse)|псевдообратную матрицу]]. Более простой вариант - применить метод [[Регуляризация по Тихонову (Tikhonov regularization)|регуляризации по Тихонову]]. Матрица $S_W + \alpha E$ всегда **обратима**.

Искомый вектор $\omega$ можно найти не только как собственный вектор матрицы $S_W^{-1}S_B$, но и более простым образом. Известно, что $S_B = (\mu_1 - \mu_2) (\mu_1 - \mu_2)^\top$, тогда:$$\lambda\omega = S_W^{-1}S_B\omega = S_W^{-1}(\mu_1 - \mu_2) (\mu_1 - \mu_2)^\top\omega.$$Выражение $(\mu_1 - \mu_2)^\top\omega$ - **скаляр** - обозначим его $c$. Выражение $S_W^{-1}(\mu_1 - \mu_2)$ - **вектор** - обозначим его $\vec{h}$. Тогда:$$\lambda\omega=c\vec{h}$$То есть вектор $\omega$ равен вектору $\vec{h}$ с некоторым числовым коэффициентом $\frac{c}{\lambda}$, который влияет на его длину и направление. **Любой** вектор, сонаправленный собственному, также является собственным и подходит для решения нашей задачи. Выразим направление вектора $\omega$ через знак пропорциональности:$$\omega \propto S_W^{-1}(\mu_1 - \mu_2)$$Такая формула справедлива только для двуклассового случая.