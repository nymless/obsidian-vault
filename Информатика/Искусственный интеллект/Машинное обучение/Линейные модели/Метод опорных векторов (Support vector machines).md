**Метод опорных векторов** (SVM) - набор алгоритмов обучения с учителем, использующихся для задач классификации и регрессионного анализа. Принадлежит к семейству [[Линейные модели|линейных моделей]].

Пусть дан набор $A$ из $n$ объектов генеральной совокупности. Каждый объект характеризуется вектором предикторов $x_i$ и относится к некоторому классу $y_i \in \{-1;1\}$, то есть $A = \big( (x_1,y_1),...,(x_2,y_2) \big)$.

Требуется построить [[Гиперплоскость (Hyperplane)|гиперплоскость]], которая разделяет объекты на два класса оптимальным образом. Разделяющая гиперплоскость задаётся уравнением:$$\langle w,x \rangle - w_0 = 0,$$где $w$ - вектор-перпендикуляр к разделяющей гиперплоскости; $w_0$ - параметр, характеризующий расстояние от гиперплоскости до начала координат. Обозначим это расстояние буквой $h$, тогда:$$h = \frac{w_0}{\lVert w \rVert_2}$$Метод опорных векторов основывается на допущении, что оптимальная разделяющая гиперплоскость должна иметь **наибольший отступ** до **ближайших точек** разных классов. Через эти точки строят две параллельные гиперплоскости по обе стороны от разделяющей гиперплоскости. Максимизируемое нами расстояние между плоскостью и гиперплоскостью называется **отступ** (**marging**). Если условиться, что ширина отступа - **разделяющая полоса**, равна $1$, то построенные гиперплоскости могут быть заданы **уравнениями**:$$\begin{gather} \langle w,x \rangle - w_0 = 1 \\ \langle w,x \rangle - w_0 = -1 \end{gather}$$При оптимальном выборе разделяющей гиперплоскости все ближайшие к ней точки разных классов находятся от неё на одинаковом удалении, остальные объекты находятся дальше и не могут попадать на **разделяющую полосу**. Таким образом для разделяющей полосы $M_i$, справедливо условие:$$M_i = y_i \big( \langle w,x_i\rangle-w_0 \big) \geq 1$$Выберем две произвольные точки классов $-1$ и $1$, лежащие на плоскостях и назовём их $x_-$ и $x_+$ соответственно. Если провести между этими точками вектор, то отступ можно интерпретировать как проекцию этого вектора на вектор нормали к гиперплоскости. Вектор между точками может быть получен вычитанием из координаты конца, координаты начала $x_+ -x_-$. Проекцию можно получить через скалярное произведение векторов. Вектор, составленный из коэффициентов $w$, является нормальным к гиперплоскости. Чтобы скалярное произведение дало именно длину проекции, необходимо исключить влияние длины вектора нормали к гиперплоскости. Для этого нормируем его поделив на $\lVert w \rVert$. Применяя [[Аксиомы скалярного произведения (Properties of an inner product)|свойство линейности]] скалярного произведения, а также заметив, что$$\begin{gather} \langle w,x_+ \rangle - w_0 = 1 \Leftrightarrow \langle w,x_+ \rangle = w_0 + 1 \\ \langle w,x_- \rangle - w_0 = -1 \Leftrightarrow \langle w,x_- \rangle = w_0 - 1  \end{gather}$$получим, что **ширина отступа** есть:$$\Big\langle (x_+ -x_-) , \frac{w}{\lVert w\rVert} \Big\rangle = \frac{\langle w,x_+ \rangle - \langle w,x_- \rangle}{\lVert w \rVert} =$$$$= \frac{(w_0 + 1) - (w_0 - 1)}{\lVert w \rVert} = \frac{2}{\lVert w \rVert} \rightarrow max$$Ширина полосы **максимальна**, когда норма вектора $w$ **минимальна**, при выполнении условия $y_i \big( \langle w,x_i\rangle-w_0 \big) \geq 1$. Это задача условной минимизации.

**Линейно разделимая выборка**

Норма вектора $w$ минимальна там же, где минимальна его [[Квадратичная форма (Quadratic form)|квадратичная форма]] вида $\langle w,w \rangle$. Построение оптимальной разделяющей гиперплоскости сводится к минимизации квадратичной формы при $m$ ограничениях-неравенствах относительно переменных $w$, $w_0$:$$\begin{cases} \langle w,w \rangle \rightarrow \min \\ y_i \big( \langle w,x_i \rangle - w_0 \big) \geq 1, \quad i=\overline{1,m}. \end{cases}$$По [[Теорема Куна-Таккера (Karush-Kuhn-Tucker conditions)|теореме Куна-Таккера]] эта задача эквивалентна двойственной задаче поиска **седловой точки Лагранжа**:$$\begin{cases}
L(w,w_0;\lambda) = \frac{1}{2}\langle w,w \rangle - \sum_{i=1}^m \lambda_i \Big( y_i \big( \langle w,x_i \rangle - w_0 \big) - 1 \Big) \rightarrow \underset{w,w_o}{\min} \underset{\lambda}{\max}\\
\lambda_i = 0, \quad \text{либо} \quad \langle w,x_i \rangle - w_0 = y_i, \quad i = \overline{1,m};\\
\lambda_i \geq 0, \quad i=\overline{1,m};\\
\end{cases}$$где $\lambda = (\lambda_1,...,\lambda_m)$ - вектор двойственных переменных.

Далее находятся частные производные [[Функция Лагранжа|лагранжиана]], из которых оказывается, что вектор переменных $w$ является линейной комбинацией обучающей выборки $x_i$, $y_i$, причём только тех, для которых $\lambda_i \neq 0$:$$w=\sum_{i=1}^m \lambda_i y_i x_i.$$Из условия $\lambda_i \neq 0$, а также из [[Теорема Куна-Таккера (Karush-Kuhn-Tucker conditions)|условий]] дополняющей нежёсткости и неотрицательности, делается вывод, что алгоритм зависит только от векторов $x_i$, для которых выполняется $\lambda_i > 0$ и $\langle w,x_i \rangle - w_0 = y_i$. Векторы, которые отвечают этим условиям, называются **опорными векторами** (**support vectors**).

Далее частные производные подставляются в лагранжиан и решается эквивалентная задача квадратичного программирования, содержащая только двойственные переменные $\lambda_i$, $y_i$, $x_i$. Получив решение задачи, можно найти переменные $w$ и $w_0$.

Решение задачи разделения линейно разделимой выборки методом SVM можно записать в **следующем виде**:$$a(x)=\text{sign} \Big( \sum_{i=1}^m \lambda_i y_i \langle x_i,x \rangle - w_0 \Big).$$Реально суммирование идёт не по всей выборке, а только по опорным вектора, для которых $\lambda_i \neq 0$. Такое свойство метода SVM принято называть **разреженностью** (**sparsity**)

**Линейно неразделимая выборка**

Для обобщения на случай линейной неразделимости, алгоритму позволяется допускать минимальные ошибки на обучающих объектах $x_i$. Введём набор дополнительных переменных $\xi_i \geq 0$, характеризующих величину ошибки. Таким образом, с введённой ошибкой, построение оптимально разделяющей гиперплоскости привод к следующим [[Теорема Куна-Таккера (Karush-Kuhn-Tucker conditions)|условиям Каруша-Куна-Таккера]]:$$\begin{cases}
L(w,w_0;\lambda) = \frac{1}{2}\langle w,w \rangle + С \sum_{i=1}^m \xi_i \rightarrow \underset{w,w_0,\xi}{\min};\\
M_i = y_i\big( \langle w,x_i \rangle - w_0 \big) \geq 1 - \xi_i, \quad i=\overline{1,m};\\
\xi_i \geq 0, \quad i=\overline{1,m}.
\end{cases}$$