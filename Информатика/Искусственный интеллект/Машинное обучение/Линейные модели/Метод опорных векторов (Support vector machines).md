**Метод опорных векторов** (SVM) - набор алгоритмов обучения с учителем, использующихся для задач классификации и регрессионного анализа. Принадлежит к семейству [[Линейные модели|линейных моделей]].

Пусть дан набор $A$ из $n$ объектов генеральной совокупности. Каждый объект характеризуется вектором предикторов $x_i$ и относится к некоторому классу $y_i \in \{-1;1\}$, то есть $A = \big( (x_1,y_1),...,(x_2,y_2) \big)$.

Требуется построить [[Гиперплоскость (Hyperplane)|гиперплоскость]], которая разделяет объекты на два класса оптимальным образом. Разделяющая гиперплоскость задаётся уравнением:$$\langle w,x \rangle - b = 0,$$где $w$ - вектор-перпендикуляр к разделяющей гиперплоскости; $b$ - параметр, характеризующий расстояние от гиперплоскости до начала координат. Обозначим это расстояние буквой $h$, тогда:$$h = \frac{b}{\lVert w \rVert_2}$$Метод опорных векторов основывается на допущении, что оптимальная разделяющая гиперплоскость должна иметь **наибольший отступ** до **ближайших точек** разных классов. Через эти точки строят две параллельные гиперплоскости по обе стороны от разделяющей гиперплоскости. Максимизируемое нами расстояние между плоскостью и гиперплоскостью называется **отступ** (marging). Построенные гиперплоскости могут быть заданы **уравнениями**:$$\begin{gather} \langle w,x \rangle - b = 1 \\ \langle w,x \rangle - b = -1 \end{gather}$$При оптимальном выборе разделяющей гиперплоскости все ближайшие к ней точки разных классов находятся от неё на одинаковом удалении, остальные объекты находятся дальше и не могут попадать на **разделяющую полосу**. Таким образом, справедливо условие:$$y_i \big( \langle w,x_i\rangle-b \big) \geq 1$$Выберем две произвольные точки классов $-1$ и $1$, лежащие на плоскостях и назовём их $x_-$ и $x_+$ соответственно. Если провести между этими точками вектор, то отступ можно интерпретировать как проекцию этого вектора на вектор нормали к гиперплоскости. Вектор между точками может быть получен вычитанием из координаты конца, координаты начала $x_+ -x_-$. Проекцию можно получить через скалярное произведение векторов. Вектор, составленный из коэффициентов $w$, является нормальным к гиперплоскости. Чтобы скалярное произведение дало именно длину проекции, необходимо исключить влияние длины вектора нормали к гиперплоскости. Для этого нормируем его поделив на $\lVert w \rVert$. Применяя [[Аксиомы скалярного произведения (Properties of an inner product)|свойство линейности]] скалярного произведения, а также заметив, что$$\begin{gather} \langle w,x_+ \rangle - b = 1 \Leftrightarrow \langle w,x_+ \rangle = b + 1 \\ \langle w,x_- \rangle - b = -1 \Leftrightarrow \langle w,x_- \rangle = b - 1  \end{gather}$$получим, что **ширина отступа** есть:$$\Big\langle (x_+ -x_-) , \frac{w}{\lVert w\rVert} \Big\rangle = \frac{\langle w,x_+ \rangle - \langle w,x_- \rangle}{\lVert w \rVert} =$$$$= \frac{(b + 1) - (b - 1)}{\lVert w \rVert} = \frac{2}{\lVert w \rVert} \rightarrow max$$Ширина полосы **максимальна**, когда норма вектора $w$ **минимальна**, при выполнении условия $y_i \big( \langle w,x_i\rangle-b \big) \geq 1$ (**задача условной минимизации**).