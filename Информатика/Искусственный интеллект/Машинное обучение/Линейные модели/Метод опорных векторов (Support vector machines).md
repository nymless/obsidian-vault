**Метод опорных векторов** (SVM) - набор алгоритмов обучения с учителем, использующихся для задач классификации и регрессионного анализа. Принадлежит к семейству [[Линейные модели|линейных моделей]].

Пусть дан набор $A$ из $n$ объектов генеральной совокупности. Каждый объект характеризуется вектором предикторов $x_i$ и относится к некоторому классу $c_i \in \{-1;1\}$, то есть $A = \big( (x_1,c_1),...,(x_2,c_2) \big)$.

Требуется построить [[Гиперплоскость (Hyperplane)|гиперплоскость]], которая разделяет объекты на два класса оптимальным образом. Разделяющая гиперплоскость задаётся уравнением:$$\langle w,x \rangle - b = 0,$$где $w$ - вектор-перпендикуляр к разделяющей гиперплоскости; $b$ - параметр, характеризующий расстояние от гиперплоскости до начала координат. Обозначим это расстояние буквой $h$, тогда:$$h = \frac{b}{\lVert w \rVert_2}$$Метод основывается на допущении, что оптимальная разделяющая гиперплоскость должна иметь наибольший зазор до точек разных классов. Для этого строят две дополнительные параллельные гиперплоскости по обе стороны от гиперплоскости разделяющей классы, до которых максимизируется **отступ**. Параллельные прямые могут быть заданы уравнениями:$$\begin{gather} \langle w,x \rangle - b = 1 \\ \langle w,x \rangle - b = -1 \end{gather}$$Тогда следующие выражения, соответствующие различным классам $c_i$, будут всегда больше нуля:$$\begin{gather} c_i \big( \langle w,x \rangle - b \big) > 0,\quad c_i = 1 \\ c_i \big( \langle w,x \rangle - b \big) > 0,\quad c_i = -1 \\ \end{gather}$$Обозначим **отступ** (marging) буквой $M_i$, тогда:$$M_i = c_i \big( \langle w,x \rangle - b \big)$$