**Lasso-регрессия** (LASSO - Least Absolute Shrinkage and Selection Operator) - метод оценивания коэффициентов [[Линейная регрессия (Linear regression)|линейной регрессионной модели]]. В отличие от Ridge-регрессии, метод использует **$L_1$-регуляризацию**. Функция потерь имеет вид:
$$
L_{Lasso}(w;y,X) = \lVert y-Xw \rVert_2^2 + \alpha\lVert w\rVert_1
$$
где $\alpha$ - коэффициент регуляризации, $\lVert \cdot \rVert_1$-$L_1$-[[L1 норма (L1 norm)|норма]].

Lasso-регрессия, также как и Ridge-регрессия, улучшает [[Численная устойчивость метода (Numerical stability)|устойчивость]] решения задачи через увеличение числа [[Обусловленность задачи (Well-conditioned, ill-conditioned problem)|обусловленности]] матрицы $X^\top X$. Однако, достигается этот результат иначе. Добавление $L_1$-регуляризации приводит к обращению в $0$ коэффициентов модели у **сильно-коррелированных** и **малоинформативных** признаков. При этом, вес одного из сильно-коррелированных признаков получается ненулевым. Это происходит из-за особенностей дифференцирования **модуля** (излом в нуле и **субградиент**).

**Недостатки** Lasso-регрессии:
Из-за особенностей дифференцирования модуля метод Lasso-регрессии плохо совместим с методом градиентного спуска. Существуют альтернативные методы оптимизации, совместимые с Lasso-регрессией.

Так как $L_1$-регуляризация является не строго выпуклым функционалом, то существует вероятность, что подпространство решений совпадёт с подпространством признака-предиктора. В таком случае решений окажется бесконечное множество, и полученная модель не будет иметь обобщающей способности.