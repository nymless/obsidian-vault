Покажем связь между **байесовским оцениванием** параметров $w$ и методом [[Регуляризация по Тихонову (Tikhonov regularization)|регуляризации по Тихонову]], применительно к решению задачи [[Линейная регрессия (Linear regression)|линейной регрессии]].

**Теорема Байеса**:
$$
P(B|A)=\frac{P(A|B)\cdot P(B)}{P(A)}
$$
В нашей задаче, для функций плотности вероятности $p$, **теорема Байеса** принимает вид:
$$
p(w|X,y)=\frac{p(y|w,X)\cdot p(w)}{p(X,y)}
$$

Примем, что известная нам **априорное плотность распределения вектора параметров** $w=(w_0,w_1,...,w_D)$ имеет **многомерное нормальное распределение** с нулевым математическим ожиданием и диагональной вариационно-ковариационной матрицей $\alpha^{-1}E_{D\times D}$ (то есть параметры не коррелированы). 

**Априорная плотность** $p(w)$ многомерного нормального распределения параметров $w \sim \mathcal{N}(0,\alpha^{-1}E)$ имеет вид:
$$
p(w) = \Big( \frac{\alpha}{2\pi} \Big)^{\frac{D+1}{2}} \exp \Big(-\frac{\alpha}{2}w^\top w \Big)
$$

Предполагаем наличие нормально распределённого шума $\varepsilon_i \sim \mathcal{N}(0,\sigma^2)$, тогда:
$$
y_i=f(x_i;w)+\varepsilon_i \Rightarrow y_i \sim \mathcal{N}(f(x_i;w),\sigma^2)
$$

**Правдоподобие** $p(y|w,X)$ - это **оценка максимального правдоподобия** (MLE - Maximum likelihood estimation) видеть данные значения $y_i$ при таких $w$ и $X$:
$$
p(y|w,X) = \prod_{i=1}^N p(y_i|x_i,w) =
$$
$$
= \Big( \frac{1}{\sqrt{2\pi}\sigma} \Big)^N \exp \Big( -\frac{1}{2\sigma^2} \sum_{i=1}^N \big( y_i - f(x_i;w) \big)^2 \Big)
$$
**Апостериорное распределение** параметра $p(w|X,y)$ пропорционально произведению правдоподобия и априорного распределения параметра. Следовательно, **максимум** апостериорного распределения достигается там же, где и **максимум** $p(y|w,X)\cdot p(w)$. Возьмём логарифм от этого выражения:
$$
\ln \Big( p(y|w,X)\cdot p(w) \Big) = \ln p(y|w,X) + \ln p(w) =
$$
$$
= \ln \Big( \frac{1}{\sqrt{2\pi}\sigma} \Big)^N - \frac{1}{2\sigma^2} \sum_{i=1}^N \big( y_i - f(x_i;w) \big)^2 +
$$
$$
+ \ln \Big( \frac{\alpha}{2\pi} \Big)^{\frac{D+1}{2}} -\frac{\alpha}{2}w^\top w \xrightarrow{w} \max
$$
Оставив только часть, которая зависит от параметров $w$, и заметив, что правдоподобие максимально, когда эта часть минимальна, получим, что **максимизация апостериорного распределения** (MAP - Maximum a posteriori estimation) параметра $w$ эквивалентна решению задачи:
$$
\frac{1}{2\sigma^2} \sum_{i=1}^N \big( y_i-f(x_i;w) \big)^2 + \frac{\alpha}{2} \lVert w\rVert^2 \xrightarrow{w} \min
$$
Данный функционал по форме совпадает с функционалом [[Регуляризация по Тихонову (Tikhonov regularization)|регуляризации по Тихонову]].

Таким образом, метод **регуляризации Тихонова** тесно связан с информацией об **априорной плотности распределения** параметров $w$ линейной модели в байесовском выводе.