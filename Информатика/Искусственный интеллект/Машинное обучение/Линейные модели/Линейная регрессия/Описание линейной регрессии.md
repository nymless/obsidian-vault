Пусть $X=(x_1,...,x_n)$ - матрица признаков, $y$ - вектор целевых признаков, $w$ - вектор параметров линейной регрессии.

**Регрессионная модель**$$y = f(x,w) + \varepsilon,$$где $\varepsilon$ - случайная ошибка (шум); называется **линейной регрессией**, если функция регрессии $f(x,w)$ имеет вид:$$f(x,w) = \langle x,w \rangle = \sum_{i=0}^n w_ix_i$$где $w_0$ - некоторое смещение модели при тождественном единице $x_0$.

**Функция потерь** и её минимизация задаются по **методу наименьших квадратов**:
$$
Loss(w;y,X) = \lVert y-Xw \rVert_2^2 = \sum_{i=1}^N (y_i-X_{i,.}w)^2 \xrightarrow{w} \min
$$

**Метод наименьших квадратов** выводится из задачи **максимизации функции правдоподобия** в [[Вероятностный подход к линейной регрессии|теоретико-вероятностной постановке]].

**Интерпретируемость метода**:
Коэффициенты линейной регрессии показывают скорость изменения зависимой переменной по данному фактору, при фиксированных остальных факторах.