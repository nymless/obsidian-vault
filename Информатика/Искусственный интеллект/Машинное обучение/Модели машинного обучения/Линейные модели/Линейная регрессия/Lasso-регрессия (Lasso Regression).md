**Lasso-регрессия** (LASSO - Least Absolute Shrinkage and Selection Operator) - метод оценивания коэффициентов [[Линейная регрессия (Linear regression)|линейной регрессионной модели]]. В отличие от Ridge-регрессии, метод использует **$L_1$-регуляризацию**. Функция потерь имеет вид:
$$
L_{Lasso}(w;y,X) = \lVert y-Xw \rVert_2^2 + \alpha\lVert w\rVert_1
$$
где $\alpha$ - коэффициент регуляризации, $\lVert \cdot \rVert_1$-$L_1$-[[L1 норма (L1 norm)|норма]].

Lasso-регрессия, также как и Ridge-регрессия, улучшает [[Численная устойчивость метода (Numerical stability)|устойчивость]] решения задачи через увеличение числа [[Обусловленность задачи (Well-conditioned, ill-conditioned problem)|обусловленности]] матрицы $X^\top X$. Однако, достигается этот результат иначе. Добавление $L_1$-регуляризации приводит к обращению в $0$ коэффициентов модели у **сильно-коррелированных** и **малоинформативных** признаков. При этом, вес одного из сильно-коррелированных признаков получается ненулевым. Это происходит из-за особенностей дифференцирования **модуля** (излом в нуле и **субградиент**).

**Недостатки** Lasso-регрессии:
Из-за особенностей дифференцирования модуля метод Lasso-регрессии плохо совместим с методом градиентного спуска. Существуют альтернативные методы оптимизации, совместимые с Lasso-регрессией.

Так как $L_1$-норма является **нестрого выпуклым** функционалом, то может оказаться, что множество оптимальных решений образует [[Линейное многообразие (Affine subspace, Linear variety)|линейное многообразие]] в пространстве параметров $w$, а не единственную точку. В терминах [[Линейная алгебра (Linear algebra)|линейной алгебры]] - система окажется [[Определённая и неопределённая система (Linear system with a single solution and Undeterminated system)|неопределённой]]. Итерационный процесс будет иметь плохую сходимость, а полученная модель не будет обладать предсказательной способностью.