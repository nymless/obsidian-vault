Метод $k$ ближайших соседей может использовать **ядерную функцию** в качестве весовой функции. В этом случае, алгоритм называется [[Ядерная оценка плотности (Kernel density estimation)|метод Парзена-Розенблатта]], или **метод парзеновского окна**.

Согласно методу [[Ядерная оценка плотности (Kernel density estimation)|ядерной оценки плотности]], внутри **окна** ядерной функции происходит оценка многомерной **плотности распределения** случайной величины. Плотности оцениваются для каждого из классов **отдельно**. Оценка плотности интерпретируется как **байесовская вероятность** $p(u|y)$ наблюдать новый объект $u=(u_1,...,u_d)$, при условии принадлежности его к классу $y \in Y$:$$p(u|y)=\frac{1}{n_yh^d}\sum_{i=1}^{n} [y_i=y] K\Big(\frac{\rho(u,x_i)}{h}\Big),$$где $[\cdot]$ - скобки Айверсона (индикаторная функция), $K$ - ядерная функция, $h$ - ширина окна, $\rho(u,x_i)$ - [[Аксиомы метрического пространства (Axioms of the metric space)|метрика расстояния]] между новой точкой $u$ и точками выборки $x_i$, принадлежащими классу $y$, $n_y$ - общее количество точек, принадлежащих классу $y$.

В зависимости от **ширины окна** различают следующие **парзеновские методы**:
- $K\big(\frac{\rho(u,x_i)}{h}\big)$ - метод парзеновского окна **фиксированной ширины** $h$. Такой метод перестаёт зависеть от гиперпараметра $k$, а только лишь от ширины окна. Формально, можно считать его не вариацией k-NN, а отдельным методом классификации. Такой метод не применим при сильном дисбалансе классов в выборке;
- $K\big(\frac{\rho(u,x_i)}{\rho(u,x_{k+1})}\big)$ - метод парзеновского окна **переменной ширины**. Такой метод зависит от гиперпараметра $k$, задавая размер окна таким образом, что ровно $k$ ближайших соседей получают ненулевые веса;
- $K\big(\frac{\rho(u,x_i)}{h(x_i)}\big)$ - метод **потенциальных функций**. В таком случае окно зависит только от обучающих объектов $x_i$ и не зависит от классифицируемого объекта $u$. Каждая точка $x_i$ как бы получает свой «потенциал». Данный метод имеет прямую физическую аналогию с электрическим потенциалом.

Оценив плотность распределения $p(u|y)$ ядерным методом, можно выразить условную вероятность того, что новый объект относится к классу $y$, то есть выразить **апостериорную вероятность** по [[Теорема Байеса|теореме Байеса]]:$$P(y|u)=\frac{p(u|y)P(y)}{P(u)}$$Нас интересует **максимизация** данного выражения для всех $y\in Y$. Максимальной вероятности будет соответствовать результат классификации. Знаменатель $P(u)$ выражения **не влияет** на максимизацию апостериорной вероятности $P(y|u)$, поэтому его опускают. Таким образом, правило классификации $y^*$ выражается как:$$y^*(u)=\arg\max_{y\in Y}p(u|y)P(y)$$**Априорная вероятность** $P(y)$ выбирается из эмпирических, экспертных, или иных соображений. Например, **эмпирическая** априорная вероятность равна:$$P(y)=\frac{n_y}{n},$$где $n_y$ - количество точек класса $y$ в обучающей выборке, $n$ - общее число точек.

Если выбрана априорная вероятность, которая является **константой** для всех классов выборки, то она **не влияет** на максимизацию функционала и её можно опустить.

**Пример**:
Классификатор объекта $u$ парзеновским методом с окном переменной ширины и эмпирической априорной вероятностью.

Эмпирическая априорная вероятность не равна для различных классов, поэтому её необходимо сохранить:$$y^*(u)=\arg\max_{y\in Y} P(y) p(u|y)=$$$$= \arg\max_{y\in Y} \frac{n_y}{n} \frac{1}{n_y\rho(u,x_{k+1})^d} \sum_{i=1}^{n} [y_i=y] K\Big(\frac{\rho(u,x_i)}{\rho(u,x_{k+1})}\Big) =$$$$= \arg\max_{y\in Y} \frac{1}{n\rho(u,x_{k+1})^d} \sum_{i=1}^{n} [y_i=y] K\Big(\frac{\rho(u,x_i)}{\rho(u,x_{k+1})}\Big)$$
Ширина окна $\rho(u,x_{k+1})$ хотя и является переменной для каждого нового объекта $u$, но, после фиксации объекта классификации, она является константой для каждого класса выборки. Следовательно вся нормирующая дробь $\frac{1}{n\rho(u,x_{k+1})^d}$ не влияет на максимизацию функционала и её можно опустить.

Таким образом, финальный вид классификатора:$$y^*(u) = \arg\max_{y\in Y} \sum_{i=1}^{n} [y_i=y] K\Big(\frac{\rho(u,x_i)}{\rho(u,x_{k+1})}\Big)$$
