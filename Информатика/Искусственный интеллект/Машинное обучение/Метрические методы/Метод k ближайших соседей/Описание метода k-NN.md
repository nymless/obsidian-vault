Пусть на множестве объектов $X$ задана некоторая функция [[Аксиомы метрического пространства (Axioms of the metric space)|расстояния]]:$$\rho: X \times X \rightarrow [0, +\infty]$$Существует целевая зависимость $y^*: X \rightarrow Y$, значения $y_i$ которой известны только на объектах обучающей выборки $X'$:$$X'=(x_i,y_i),\quad y_i=y^*(x_i) \in Y,\quad i=\overline{1,l}.$$Множество классов $Y$ конечно. Требуется построить алгоритм классификации $a: X \rightarrow Y$, аппроксимирующий целевую зависимость $y^*(x)$ на всём множестве $X$.

Предположим, что требуется классифицировать новый объект $u \in X$. Для этого найдём $k$ наиболее близких к $u$ (в смысле расстояния $\varrho$) объектов обучающей выборки.

Расположим элементы обучающей выборки $x_1,...,x_l$ в порядке возрастания до $u$:$$\rho(u,x_u^{(1)}) \leq \rho(u,x_u^{(2)}) \leq ... \rho(u,x_u^{(l)}),$$где через $x_u^{(i)}$ обозначается $i$-й сосед объекта $u$. Соответственно, ответ на $i$-м соседе объекта $u$ есть:$$y_u^{(i)} = y^*(x_u^{(i)}).$$Фактически, любой объект $u$ порождает свою перенумерацию выборки:$$X'=\{ (x_u^{(1)},y_u^{(1)}),...,(x_u^{(l)},y_u^{(l)}) \}$$Возникшие конфликты для равных расстояний можно разрешать произвольным образом.

Тогда алгоритм классификации **$k$ ближайших соседей** можно выразить следующим образом:$$a(u,X')=\arg\max_{y\in Y}\sum_{i=1}^l[y_u^{(i)}=y]w(i,u),$$где $[\cdot]$ - **скобки Айверсона** - равны $1$ в случае истинности и $0$ в противном случае; $w(i,u)$ - заданная весовая функция, которая оценивает степень важности $i$-го соседа для классификации объекта $u$.

По разному задавая весовую функцию, можно получить различные варианты метода k-NN:
- $w(i,u)=[i=1]$ - метод одного ближайшего соседа;
- $w(i,u)=[i\leq k]$ - метод $k$ ближайших соседей;
- $w(i,u)=[i\leq k]\frac{k+1-i}{k}$ - метод $k$ **линейно взвешенных** ближайших соседей;
- $w(i,u)=[i\leq k]q^i$ - метод $k$ **экспоненциально взвешенных** ближайших соседей, где $q\in(0,1)$;
- $w(i,u)=[i\leq k] \Big(-\exp\big(\rho(u,x_u^{(i)})\big)\Big)$ - метод, использующий **обратную экспоненту** от расстояния;
- [[k-NN и метод парзеновского окна|Метод парзеновского окна]].

Число $k$ - **гиперпараметр метода** k-NN. На практике оптимальное значение параметра $k$ определяют по критерию **скользящего контроля** с исключением объектов по одному (leave-one-out cross-validation, LOO CV). Для каждого объекта $x_i \in X'$ проверяется, правильно ли он классифицируется по своим $k$ ближайшим соседям:$$\text{LOO}(k,X')=\sum_{i=1}^l \Big[ a \big( x_i;X' - \{x_i\},k \big) \neq y_i \Big] \rightarrow \min_k$$Если классифицируемый объект $x+i$ не исключать из обучающей выборки, то ближайшим соседом $x_i$ всегда будет сам $x_i$, и минимальное (нулевое) значение функционала $\text{LOO}$ будет достигаться при $k=1$.