#Книги #DataScience #Заметки

**Оригинал**: Data Science from Scratch, 2nd edition (2019)
**Автор**: Джоэль Грас (Joel Grus)

## Заметки

1. В **Python** операция `in` использованная на множестве `set` работает **намного быстрее**, чем на списках `list` и кортежах `tuple`. Справедливо даже с учетом предварительного создания множества из списка. Например:
```python
items = [...]  # Список строковых элементов

if "item" in set(items):  # Такой код выполнится быстрее
	pass
if "item" in items:  # Такой код выполнится медленнее
	pass
```
2. Использование инструкции `assert` вместо инструкции `raise` там, где есть логика **проверки условия**. В этом случае будет подниматься ошибка `AssertionError`, что ограничивает гибкость обработки ошибок. Однако в некоторых случаях можно применять для улучшения выразительности кода.
3. Как посчитать **длину вектора**, когда известны его координаты? Найти квадрат вектора и взять из него корень. **Квадрат вектора** - это скалярное произведение вектора самого на себя, т.е. сумма квадратов всех его элементов. Метод нахождения длины вектора - это просто расширенная **теорема Пифагора**.
4. Рассматривается **не взвешенный граф** связей элементов (например, дружеские связи между людьми). Требуется как-то представить эти связи в виде пригодном для компьютерной программы. Если связей очень много, то создание пар в виде списка кортежей не лучшая идея, т.к. для выявления связи между двумя элементами придётся перебрать весь список. Лучше составить **матрицу смежности** (**adjacency matrix**), т.е. такую матрицу пар всех вершин (рёбер), где 1 означает наличие связи, а 0 её отсутствие. Так мы сможем проверять наличие связи между двумя вершинами графа **намного быстрее**. При этом мы будем вынуждены хранить разреженную матрицу, что потребует **больше памяти**.
5. **Выборочное среднее** (**Mean**) - это статистика очень чувствительная к выбросам в данных. Одним из вариантов более устойчивой статистики может выступить **медиана** (**median**). Нужно сортировать все элементы. Тогда для нечётного количества элементов медиана будет являться средним арифметическим всех порядковых номеров. Если медиана сильно отличается от среднего, то это признак необходимости **удаления выбросов** в данных.
6. **Выборочная дисперсия** (**Variance**) - это сумма квадратов отклонений от среднего значения, делённая на количество элементов.$$var = \frac1n\sum_{i=1}^n(x_i-\overline{x})^2$$Геометрический смысл дисперсии - это **квадрат вектора** (см. пункт 3) отклонений от средних значений, делённый на $n$ (или $n-1$ для несмещённой дисперсии).
7. **Стандартное отклонение** (**Standard deviation**), или иначе - среднеквадратическое отклонение - это квадратный корень из дисперсии:$$\sigma = \sqrt{var}, \quad тогда \quad var = \sigma^2$$Геометрический смысл стандартного отклонения - это **длина вектора** отклонений от средних значений, делённая на $\sqrt{n}$:$$\sigma = \frac{1}{\sqrt{n}}\lVert d \rVert,\quad где \quad \lVert d \rVert = \sqrt{\sum_{i=1}^n(x_i-\overline{x})^2}$$
8. **Ковариация** (**Covariance**) - это скалярное произведение векторов двух отклонений от их средних значений, делённое на количество элементов.$$cov(x,y) = \frac1n\sum_{i=1}^n(x_i-\overline{x})(y_i-\overline{y})$$Ковариацию можно считать **парным аналогом** дисперсии, где происходит скалярное умножение вектора самого на себя.
9. **Коэффициент корреляции** (**Correlation coefficient**) - это ковариация двух векторов, масштабированная в диапазоне от -1 до 1. Для коэффициента **корреляции Пирсона** это достигается путём деления ковариации на произведение стандартных отклонений:$$r=\frac{cov(x,y)}{\sigma_X\sigma_Y}$$Коэффициент корреляции характеризует насколько значения одного вектора характеризуют значения другого. Иначе какова **теснота** линейной связи между двумя векторами. **Сила** линейной связи (наклон) характеризуется **коэффициентом регрессии** $\beta$, который связан с коэффициентом корреляции $r$ следующим соотношением: $$r = \beta \frac{\sigma_X}{\sigma_Y}$$Если же связь нелинейная, например квадратичная, то ковариация может оказаться равной нулю, следовательно коэффициент корреляции будет равен нулю, следовательно и коэффициент регрессии также будет равен нулю. Линия регрессии при этом будет горизонтальна - линейной зависимости нет.
10. **Парадокс Симпсона** - это статистический феномен, при котором наблюдаемая корреляция между двумя переменными меняет направление (или исчезает) при учёте **спутывающей переменной** (confounding variable). Он возникает в ситуациях, когда данные агрегируют по группам, сформированным по одному признаку, а затем оказывается, что предварительная агрегация по другому (пропущенному) признаку даёт противоположную картину. Такой признак может существенно влиять на поведение всей системы. Игнорирование его приводит к **ложным выводам** в корреляционном анализе.
11. **Ошибка причинности** - это ошибка интерпретации, когда на основе наличия значимой корреляции делается вывод о причинно-следственной связи. Такого вывода делать нельзя, т.к. не известно направление причинной связи. Также причинная связь может существовать через третью скрытую переменную. Причинная связь может отсутствовать вовсе, а корреляция проявилась в результате случайной флуктуации.
12. **Условная вероятность** - это вероятность наступления одного события, при условии наступления второго события. Если есть два  взаимно **независимых** события $A$ и $B$, то вероятность их взаимного наступления равна произведению вероятностей наступления каждого:$$P(A\cap B)=P(A)\:P(B)$$Если события $A$ и $B$ являются **зависимыми**, то$$P(A\cap B)=P(A|B)\:P(B)=P(B|A)\:P(A)$$Следовательно, **условная вероятность** равна:$$P(A|B)=\frac{P(A\cap B)}{P(B)}$$
13. События $H_1,H_2,...,H_n$ образуют **полную группу событий** если они попарно несовместны, а их объединение составляет всё вероятностное пространство $\Omega$. Т.е.$$P(H_1)+P(H_2)+...+P(H_n)=P(\Omega)=1$$Тогда вероятность любого события $A$ называется **полной вероятностью** и находится по формуле:$$P(A)=\sum_{i=1}^n P(A|H_i)\;P(H_i)$$
14. **Теорема Байеса** (**Bayes theorem**) - если есть полная группа событий $H_1,H_2,...,H_n$ (гипотезы) и некоторое событие $A$, то условная вероятность того, что имело место событие $H_k$, если произошло событие $A$, находится по формуле:$$P(H_k|A)=\frac{P(A|H_k)\;P(H_k)}{P(A)}=\frac{P(A|H_k)\;P(H_k)}{\sum_{i=1}^n P(A|H_i)\;P(H_i)},$$где $P(H)$ - **априорная** вероятность; $P(H|A)$ - **апостериорная** вероятность; $P(A|H)$ - вероятность события $A$ при условии наступления события $B$; $P(A)$ - полная вероятность события $A$.
15. **Статистические гипотезы** - предварительно сформированные утверждения, значимость которых проверяется статистическими методами. Выделяется главная, или **нулевая гипотеза** $H_0$,  которая представляет некоторую позицию по умолчанию, и **альтернативная гипотеза** $H_1$, относительно которой мы хотим её сопоставить. При проверке статистических гипотез выделяют следующие возможные ошибки: **ошибка 1-го рода** ("ложное утверждение"), при котором мы отклоняем верную нулевую гипотезу $H_0$, и **ошибка 2-го рода** ("ложное отрицание"), когда нам не удалось отклонить ложную нулевую гипотезу $H_0$.
16. **Проверка статистических гипотез** - заключается в выборе **статистического критерия**, распределение которого заранее известно, и последующем расчёте **критической области**, достигнув которой мы отклоняем нулевую гипотезу. Для расчёта критической области предварительно требуется выбрать значимость. **Уровень значимости** - это вероятность допустить ошибку 1-го рода. Значимость часто выбирается на уровне 5%, или 1%. От выбранного уровня значимости также зависит и вероятность допустить ошибку 2-го рода. **Мощность критерия** - это вероятность принятия альтернативной гипотезы $H_1$, при условии её верности. Если обозначить вероятность ошибки 2-го рода буквой $\beta$, то мощность критерия равна: $1-\beta$.
17. **P-значение** (**P-value**) - альтернативный подход к проверке статистических гипотез - это вероятность получить такое, или ещё более экстремальное отклонение критерия, при условии справедливости гипотезы $H_0$. Рассчитанное p-значение сравнивается с **уровнем значимости** и принимается решение об отклонении, или не отклонении нулевой гипотезы.
18. **Взлом p-значения** (**P-hacking**) - если проводить проверку множества неверных гипотез, то среди них почти наверняка будет получено достаточно низкое p-value для принятия альтернативной гипотезы (ошибка 1-го рода). Такого же эффекта можно добиться при проведении достаточного количества экспериментов для принятия альтернативной гипотезы. Также для эффекта взлома p-значения может быть достаточно удаления из выборки "правильных выбросов" - значений которые не являются выбросами.
19. **Доверительный интервал** (**Confidence interval**) - это диапазон значений неизвестного параметра распределения, содержащий истинное значение параметра с заданной вероятностью, которая называется **достоверность**, надёжность, или уверенность. Доверительный интервал применяется для подтверждения статистической значимости точечной оценки параметра, которая может быть смещённой. На практике достоверность часто задаётся на уровне 95%. В этом случае говорят о "95%-ом доверительном интервале". Задача нахождения 95%-го доверительного интервала заключается в нахождении 2.5% и 97.5% перцентилей распределения неизвестного параметра.
20. **A/B-тестирование** - метод маркетингового исследования, позволяющий на основе статистики оценить влияние изменения на метрики продукта. A/B-тест состоит из рандомизированного контролируемого эксперимента, при котором субъекты случайно распределяются в две группы A и B. Эти группы получают разные варианты продукта A и B, которые сравниваются с целью определения лучшего. Обычно для сравнения выбирается нулевая статистическая гипотеза о равенстве средних двух распределений значений метрики продукта.
21. **Градиентный спуск** (**Gradient descent**) - итеративный метод оптимизации, т.е. нахождения **экстремумов** функции. В рамках машинного обучения обычно находят **минимум** функции потерь (loss), или же **максимум** функции правдоподобия (likelihood). Метод заключается в нахождении **частных производных** оптимизируемой функции по каждой из переменных (предикторов). Полученный вектор частных производных называется **градиентом** и характеризует направление наискорейшего роста функции. Следовательно, двигаясь с определённым шагом по направлению градиента можно найти максимум оптимизируемой функции. Двигая против градиента (по антиградиенту) можно найти минимум функции.
22. **Пакетный градиентный спуск** - итеративный метод оптимизации на основе градиентного спуска, при котором для вычисления вектора градиента используется сумма градиентов по всему набору данных.
23. **Мини-пакетный градиентный спуск** - итеративный метод оптимизации, при котором для вычисления вектора градиента используется не весть набор данных, а лишь случайная подвыборка, или мини-пакет (mini-batch). Это обеспечивает баланс вычислительной сложности и стабильности.
24. **Стохастический градиентный спуск** (**Stohastic gradient descent**) - итеративный метод оптимизации, при котором для вычисления вектора градиента используется не весть набор данных, а лишь один случайный элемент выборки, что существенно сокращает вычислительные затраты. Также для выбора одного элемента из выборки можно использовать различные эвристики, эффективность которых проверяется эмпирически. Из преимуществ метода ещё можно выделить то, что одно значение обладает своим собственным шумом. Это может способствовать выходу итеративного процесса оптимизации из локальных экстремумов без использования дополнительных техник.
25. **Масштабирование** (**Scaling**) - это преобразование значений параметров путём изменения их масштаба, не изменяя их внутренней структуры. Различают несколько техник масштабирования, например min-max масштабирование в заданном диапазоне, или стандартизацию, подразумевающую приведение среднего к 0 и стандартного отклонения к 1. Масштабирование данных необходимо для улучшения сходимости при обучении моделей, а так же там, где необходимо сравнение метрик расстояния, например в кластеризации. Например, для сравнения евклидова, или манхэттенского расстояния, может потребоваться предварительная стандартизация данных. Для сравнения косинусного расстояния между векторами, может потребоваться предварительная $L_2$-нормализация векторов, которая не относится к масштабированию признаков, т.к. нарушает распределение.
26. 